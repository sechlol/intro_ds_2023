I'm writing an academic data science blog post about a ML project. It's about predicting the stock market profitability. You have to write the paragraphs based on my precise instructions. For each point, write one or more paragraphs for a general audience. Be professional, but don't use weird uncommon terminology. 

STRUCTURE:
- Discuss the prediction goal 
- Discuss the data augmentation
For each algorithm (XGBoost, MLP, LSTM/GRU)
    - Write Short description about the algorithm
    - Discuss Hyperparameters setup
    - Discuss Convergence plot (if available), 
    - Discuss Confusion matrix on test and train set
    - Discuss the accuracy metrics.
- Discuss the results as whole

CONTENT:
Prediction goal:
    - Question: "Can you become a billionaire with ML?"
    - Predict if stock market will be profitable in X future periods (with tunable X)
    - Binary classification: For each day, predict 1 if price at time +X will be higher than today, else 0

Data Augmentation:
    - Original dataset comprises 9 of the 11 major market sector indices, and other US macroeconomic data
    - Before training, we augment the data by calculating: relative strenght between indices, SMA at 20, 50, 200 periods, relative returns

XGBoost:
    - Hyperparameter setup: 
        - we use binary:logistic as prediction objective, meaning that the output will be the probability af a positive return between 0 to 1
        - evaluation metrics to compute quality of predictions are: "error" (binary classification error), "auc" (Area under the curve), "logloss" (negative log likelihood)
        - 5-fold cross validation
        - Trees use "hist" algorithm (Faster histogram optimized approximate greedy algorithm) with max_depth=3 and min_child_weight=4
        - Early stopping if no changes in 20 rounds
    - Convergence plot shows decreasing errors with each iteration, and increasing AUC
    - TRAIN SET Confusion matrix: slightly biased towards false negtives. {'accuracy': 0.8244406196213425, 'accuracy_dummy': 0.6110154905335629, 'f1_score': 0.8619756427604871, 'recall': 0.8971830985915493, 'precision': 0.8294270833333334, 'r2_score': 0.2613486227097095}

    - TEST SET Confusion matrix: slightly biased towards false negtives. {'accuracy': 0.4675324675324675, 'accuracy_dummy': 0.474025974025974, 'f1_score': 0.6339285714285714, 'recall': 0.9726027397260274, 'precision': 0.47019867549668876, 'r2_score': -1.135633350245223}

MLP:
    - Hyperparameter setup: 
        - 100 hidden layers
        - SGD optimizer with constant learning rate
        - batch size 64
        - Early stopping at 20 unchanged iterations
    - TRAIN SET confusion matrix: balanced amount of false positive and negatives.  {'accuracy': 0.7306368330464716, 'accuracy_dummy': 0.6110154905335629, 'f1_score': 0.7955584585238406, 'recall': 0.8577464788732394, 'precision': 0.7417783191230207, 'r2_score': -0.13332294652873}

    - TEST SET Confusion matrix: biased towards false positives.  {'accuracy': 0.5, 'accuracy_dummy': 0.474025974025974, 'f1_score': 0.41813602015113344, 'recall': 0.3789954337899543, 'precision': 0.46629213483146065, 'r2_score': -1.005411804498563}

LSTM / GRU:
    - Hyperparameter setup: We tried with several models, either LSTM and GRU
        - 50 layers with tanh activation function, followed by a unit normalization layer, ending with a 3-layer dense classifier
        - SGD optimizer with learning rate of 0.01
        - BinaryCrossentropy as loss function
        - BinaryAccuracy as metric
    - TRAIN SET confusion matrix: heavily biased towards false negatives.  {'binary_accuracy': 0.648531973361969, 'accuracy': 0.6485319516407599, 'accuracy_dummy': 0.6008635578583765, 'f1_score': 0.7348534201954398, 'recall': 0.8105777522276516, 'precision': 0.6720686367969495, 'r2_score': -0.46550938193915914}

    - TEST SET confusion matrix: heavily biased towards false negatives.  {'binary_accuracy': 0.46726861596107483, 'accuracy': 0.4672686230248307, 'accuracy_dummy': 0.48081264108352145, 'f1_score': 0.5241935483870968, 'recall': 0.6103286384976526, 'precision': 0.45936395759717313, 'r2_score': -1.1340681771790155}


Overall Results:
Accuracy on test set were generally good, but all the algorithms were overfitting, because when predicting the test dataset the accuracy was no better than the dummy model. 



